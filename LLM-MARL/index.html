<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LAMARL: LLM-Aided Multi-Agent Reinforcement Learning</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header class="center-content">
        <h1>LAMARL: LLM-Aided Multi-Agent Reinforcement Learning</h1>
        <p><strong>Authors:</strong></p>
        <div class="author">
            <span class="author-name">Guobin Zhu<sup class="superscript">1,2</sup></span>
            <span class="author-name">Rui Zhou<sup class="superscript">1</sup></span>
            <span class="author-name">Wenkang Ji<sup class="superscript">2</sup></span>
            <span class="author-name">Shiyu Zhao<sup class="superscript">2</sup></span>
        </div>
        <div class="affiliation">
            <p><strong>Affiliations:</strong></p>
            <p><sup>1</sup> School of Automation Science and Electrical Engineering, Beihang University, Beijing, China</p>
            <p><sup>2</sup> WINDY Lab, School of Engineering at Westlake University, Hangzhou, China</p>
        </div>
        <div class="code-link">
            <button onclick="window.open('https://github.com/Guobin-Zhu/MARL-LLM', '_blank')">Code</button>
        </div>
    </header>
    <section id="abstract" class="center-content abstract-content">
        <h2>Abstract</h2>
        <p>Although Multi-Agent Reinforcement Learning (MARL) is suitable for addressing complex multi-robot system tasks, it suffers from low sample efficiency and requires frequent manual tuning of reward functions. Large Language Models (LLMs) have shown promise in single-robot scenarios, but their application in multi-robot systems remains largely unexplored. This paper introduces a novel LLM-aided MARL (LAMARL) approach, which integrates MARL with LLMs in two key parts. First, the LLM is used to design a prior policy, which is incorporated into the actor loss of MARL to equip robots with basic task capabilities. Second, the LLM is used to design a reward function, where basic APIs and the Chain-of-Thought (CoT) mechanism in the prompt enable more accurate and reliable outputs. In complex exploration tasks, comparative experiments demonstrate that LAMARL achieves near-optimal performance without the need for manual design. Ablation studies show that the prior policy improves sample efficiency by over 2 times and significantly aids in task completion. Real-world experiments further confirm the feasibility of LAMARL.</p>
    </section>

    <!-- Section for Image -->
    <section id="image" class="center-content">
        <img src="images/framework.jpg" width="800" height="450" alt="PDF Image" class="center-image">
    </section>

    <section id="methodology" class="center-content">
        <h2>Methodology</h2>
        <p>Write your methodology here.</p>
        
        <section id="llm-designed-reward" class="center-content left-align">
            <h3>LLM-designed Reward</h3>
            <p>Write about LLM-designed reward here.</p>
        </section>
        
        <section id="llm-designed-prior-policy" class="center-content left-align">
            <h3>LLM-designed Prior Policy</h3>
            <p>Write about LLM-designed prior policy here.</p>
        </section>

    </section>
    <section id="results" class="center-content">
        <h2>Results</h2>
        <p>Write your results here.</p>
    </section>
    <footer class="center-content">
        <p>&copy; Year Guobin Zhu</p>
    </footer>
</body>
</html>
